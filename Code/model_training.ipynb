{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Current Device: 0\n",
      "Total GPU Memory (MB): 4095\n",
      "Result from GPU: tensor([4., 6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current Device:\", torch.cuda.current_device())\n",
    "    print(\"Total GPU Memory (MB):\", torch.cuda.get_device_properties(0).total_memory // (1024 ** 2))\n",
    "\n",
    "    # Test simple GPU operation\n",
    "    x = torch.tensor([1.0, 2.0], device=\"cuda\")\n",
    "    y = torch.tensor([3.0, 4.0], device=\"cuda\")\n",
    "    z = x + y\n",
    "    print(\"Result from GPU:\", z)\n",
    "else:\n",
    "    print(\"No GPU detected by PyTorch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import WinDLL\n",
    "\n",
    "# Should succeed silently if DLL is found\n",
    "WinDLL(\"cudart64_110.dll\")\n",
    "WinDLL(\"cudnn64_8.dll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"✅ GPU setup complete.\")\n",
    "setup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess .mp4 videos\n",
    "def load_video(video_path, max_frames=30, target_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path.numpy().decode(\"utf-8\"))\n",
    "    frames = []\n",
    "    \n",
    "    for _ in range(max_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)  # Resize for MoViNet\n",
    "        frame = frame / 255.0  # Normalize pixels\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Pad if fewer than max_frames\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(np.zeros(target_size + (3,)))\n",
    "\n",
    "    return np.array(frames, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names(data_dir):\n",
    "    class_names = sorted([\n",
    "        folder for folder in os.listdir(data_dir)\n",
    "        if os.path.isdir(os.path.join(data_dir, folder))\n",
    "    ])\n",
    "    print(\"Label Mapping (index → class):\")\n",
    "    for idx, name in enumerate(class_names):\n",
    "        print(f\"{idx:2d} → {name}\")\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def create_dataset(directory, batch_size=8):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for f in os.listdir(class_dir):\n",
    "            if f.endswith(\".mp4\"):\n",
    "                video_paths.append(os.path.join(class_dir, f))\n",
    "                labels.append(idx)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n",
    "\n",
    "    def _parse_function(filename, label):\n",
    "        video = tf.py_function(load_video, [filename], tf.float32)\n",
    "        video.set_shape([30, 224, 224, 3])\n",
    "        return video, label\n",
    "\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = get_class_names(\"../Dataset/split_data/train\")\n",
    "num_classes = len(class_names)\n",
    "\n",
    "train_data = create_dataset(\"../Dataset/split_data/train\", batch_size=8)\n",
    "val_data = create_dataset(\"../Dataset/split_data/val\", batch_size=8)\n",
    "test_data = create_dataset(\"../Dataset/split_data/test\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all training labels\n",
    "train_labels = []\n",
    "for idx, cls in enumerate(class_names):\n",
    "    count = len(os.listdir(os.path.join(\"../Dataset/split_data/train\", cls)))\n",
    "    train_labels += [idx] * count\n",
    "\n",
    "# Compute balanced class weights\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.arange(len(class_names)), y=train_labels)\n",
    "class_weights = {i: w for i, w in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MoViNet-A3 from TensorFlow Hub\n",
    "model_id = 'a3'\n",
    "mode = 'base'\n",
    "version = '3'\n",
    "model_url = f\"https://tfhub.dev/tensorflow/movinet/{model_id}/{mode}/kinetics-600/classification/{version}\"\n",
    "\n",
    "# input_layer = layers.Input(shape=(30, 224, 224, 3), name=\"input_video\")\n",
    "inputs = keras.Input(shape=(30, 224, 224, 3), name=\"input_video\")\n",
    "\n",
    "class MoViNetFeatureExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self, hub_url, trainable=False):\n",
    "        super().__init__()\n",
    "        self.movinet_layer = hub.KerasLayer(hub_url, trainable=trainable)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expecting input of shape (None, 30, 224, 224, 3)\n",
    "        return self.movinet_layer({'image': inputs})\n",
    "\n",
    "x = MoViNetFeatureExtractor(model_url,trainable=True)(inputs)\n",
    "\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lr_callback = ReduceLROnPlateau (\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_callback]\n",
    ")\n",
    "\n",
    "model.save(\"movinet_final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

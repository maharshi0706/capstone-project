{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11393554,"sourceType":"datasetVersion","datasetId":7135518},{"sourceId":11439639,"sourceType":"datasetVersion","datasetId":7165989},{"sourceId":11449886,"sourceType":"datasetVersion","datasetId":7173711},{"sourceId":11466016,"sourceType":"datasetVersion","datasetId":7185283},{"sourceId":344816,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":288241,"modelId":309019},{"sourceId":344857,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":288266,"modelId":309043}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# tf ->2.19.0 | 2.18.0\n!pip install -U -q \"tf-models-official\"\n# !pip show tf-models-official","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:10.091341Z","iopub.execute_input":"2025-04-18T21:32:10.091680Z","iopub.status.idle":"2025-04-18T21:32:13.944373Z","shell.execute_reply.started":"2025-04-18T21:32:10.091656Z","shell.execute_reply":"2025-04-18T21:32:13.943327Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install keras==3.5.0\n!pip uninstall keras -y\n\n# !pip uninstall tensorflow -y\n# !pip install tensorflow==2.17.1\n\n!pip install -U tf_keras # Keras 2\nimport os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:13.946158Z","iopub.execute_input":"2025-04-18T21:32:13.946453Z","iopub.status.idle":"2025-04-18T21:32:19.688967Z","shell.execute_reply.started":"2025-04-18T21:32:13.946432Z","shell.execute_reply":"2025-04-18T21:32:19.688268Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: keras 3.9.2\nUninstalling keras-3.9.2:\n  Successfully uninstalled keras-3.9.2\nRequirement already satisfied: tf_keras in /usr/local/lib/python3.11/dist-packages (2.19.0)\nRequirement already satisfied: tensorflow<2.20,>=2.19 in /usr/local/lib/python3.11/dist-packages (from tf_keras) (2.19.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.70.0)\nRequirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\nCollecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf_keras)\n  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.12.1)\nRequirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.14.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.2.0,>=1.26.0->tensorflow<2.20,>=2.19->tf_keras) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.2)\nUsing cached keras-3.9.2-py3-none-any.whl (1.3 MB)\nInstalling collected packages: keras\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.9.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# START OF LATEST CODE","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport pickle\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras import mixed_precision\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom official.projects.movinet.modeling import movinet, movinet_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:19.689937Z","iopub.execute_input":"2025-04-18T21:32:19.690192Z","iopub.status.idle":"2025-04-18T21:32:24.655855Z","shell.execute_reply.started":"2025-04-18T21:32:19.690169Z","shell.execute_reply":"2025-04-18T21:32:24.655025Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-04-18 21:32:20.982792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745011941.006644   68357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745011941.013873   68357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1745011941.034936   68357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745011941.034954   68357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745011941.034957   68357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745011941.034959   68357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Disable mixed precision temporarily\nmixed_precision.set_global_policy('float32')","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:24.656773Z","iopub.execute_input":"2025-04-18T21:32:24.657274Z","iopub.status.idle":"2025-04-18T21:32:24.661220Z","shell.execute_reply.started":"2025-04-18T21:32:24.657228Z","shell.execute_reply":"2025-04-18T21:32:24.660448Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_class_names(data_dir):\n    class_names = sorted([folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))])\n    print(f\"Class names in {data_dir}:\", class_names)\n    return class_names\n\n# Check all datasets\ntrain_class_names = get_class_names(\"/kaggle/input/capstone-dataset-updated/split_data_2/train\")\nval_class_names = get_class_names(\"/kaggle/input/capstone-dataset-updated/split_data_2/val\")\ntest_class_names = get_class_names(\"/kaggle/input/capstone-dataset-updated/split_data_2/test\")\n\n# Ensure consistency\nassert train_class_names == val_class_names == test_class_names, \"Class names mismatch across datasets!\"\nclass_names = train_class_names\nnum_classes = len(class_names)\nprint(\"Number of classes:\", num_classes)","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:24.663043Z","iopub.execute_input":"2025-04-18T21:32:24.663265Z","iopub.status.idle":"2025-04-18T21:32:24.718176Z","shell.execute_reply.started":"2025-04-18T21:32:24.663228Z","shell.execute_reply":"2025-04-18T21:32:24.717498Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Class names in /kaggle/input/capstone-dataset-updated/split_data_2/train: ['Corner', 'Direct free-kick', 'Foul', 'Goal', 'Indirect free-kick', 'Kick-off', 'Offside', 'Shots off target', 'Shots on target', 'Substitution', 'Throw-in', 'Yellow card']\nClass names in /kaggle/input/capstone-dataset-updated/split_data_2/val: ['Corner', 'Direct free-kick', 'Foul', 'Goal', 'Indirect free-kick', 'Kick-off', 'Offside', 'Shots off target', 'Shots on target', 'Substitution', 'Throw-in', 'Yellow card']\nClass names in /kaggle/input/capstone-dataset-updated/split_data_2/test: ['Corner', 'Direct free-kick', 'Foul', 'Goal', 'Indirect free-kick', 'Kick-off', 'Offside', 'Shots off target', 'Shots on target', 'Substitution', 'Throw-in', 'Yellow card']\nNumber of classes: 12\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# # Video loading function with RGB conversion and additional augmentations\n# def load_video(video_path, max_frames=16, target_size=(224, 224), augment=False, frame_stride=1):\n#     video_path = video_path.numpy().decode(\"utf-8\")\n#     cap = cv2.VideoCapture(video_path)\n#     frames = []\n#     frame_count = 0\n#     while len(frames) < max_frames:\n#         ret, frame = cap.read()\n#         if not ret or frame is None:\n#             break\n#         if frame_count % frame_stride == 0:\n#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#             if augment:\n#                 if np.random.rand() < 0.5:\n#                     h, w = frame.shape[:2]\n#                     crop_h, crop_w = int(h * 0.9), int(w * 0.9)\n#                     y = np.random.randint(0, h - crop_h)\n#                     x = np.random.randint(0, w - crop_w)\n#                     frame = frame[y:y+crop_h, x:x+crop_w]\n#                 if np.random.rand() < 0.3:\n#                     angle = np.random.uniform(-10, 10)\n#                     M = cv2.getRotationMatrix2D((frame.shape[1]/2, frame.shape[0]/2), angle, 1)\n#                     frame = cv2.warpAffine(frame, M, (frame.shape[1], frame.shape[0]))\n#             frame = cv2.resize(frame, target_size)\n#             frame = frame / 255.0\n#             if augment:\n#                 if np.random.rand() < 0.5:\n#                     frame = cv2.flip(frame, 1)\n#                 if np.random.rand() < 0.5:\n#                     factor = 0.8 + 0.4 * np.random.rand()\n#                     frame = np.clip(frame * factor, 0, 1)\n#             frames.append(frame)\n#         frame_count += 1\n#     cap.release()\n#     while len(frames) < max_frames:\n#         frames.append(np.zeros(target_size + (3,), dtype=np.float32))\n#     video = np.array(frames, dtype=np.float32)\n#     if np.any(np.isnan(video)):\n#         raise ValueError(f\"NaN values found in video: {video_path}\")\n#     return tf.convert_to_tensor(video)\n\ndef load_video(video_path, max_frames=16, target_size=(224, 224), augment=False, frame_stride=2):\n    try:\n        video_path = video_path.numpy().decode(\"utf-8\")\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            print(f\"Error: Cannot open video: {video_path}\")\n            return tf.zeros([max_frames, target_size[0], target_size[1], 3], dtype=tf.float32)\n        frames = []\n        frame_count = 0\n        valid_frames = 0\n        while len(frames) < max_frames:\n            ret, frame = cap.read()\n            if not ret or frame is None:\n                break\n            if frame_count % frame_stride == 0:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                if augment and np.random.rand() < 0.5:  # Reduced augmentation frequency\n                    if np.random.rand() < 0.6:\n                        h, w = frame.shape[:2]\n                        crop_h, crop_w = int(h * 0.9), int(w * 0.9)\n                        y = np.random.randint(0, h - crop_h)\n                        x = np.random.randint(0, w - crop_w)\n                        frame = frame[y:y+crop_h, x:x+crop_w]\n                frame = cv2.resize(frame, target_size)\n                frame = frame / 255.0\n                frames.append(frame)\n                valid_frames += 1\n            frame_count += 1\n        cap.release()\n        while len(frames) < max_frames:\n            frames.append(np.zeros(target_size + (3,), dtype=np.float32))\n        video = np.array(frames, dtype=np.float32)\n        if valid_frames == 0:\n            print(f\"Warning: No valid frames in video: {video_path}\")\n            return tf.zeros([max_frames, target_size[0], target_size[1], 3], dtype=tf.float32)\n        if np.any(np.isnan(video)) or np.all(video == 0):\n            print(f\"Error: Invalid video (NaN or all-zero): {video_path}\")\n            return tf.zeros([max_frames, target_size[0], target_size[1], 3], dtype=tf.float32)\n        return tf.convert_to_tensor(video)\n    except Exception as e:\n        print(f\"Error processing video {video_path}: {str(e)}\")\n        return tf.zeros([max_frames, target_size[0], target_size[1], 3], dtype=tf.float32)\n\n\ndef get_class_names(data_dir):\n    class_names = sorted([folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))])\n    print(class_names)\n    return class_names\n\n# def create_dataset(directory, batch_size, class_names, augment=False):\n#     video_paths, labels = [], []\n#     for idx, class_name in enumerate(class_names):\n#         class_dir = os.path.join(directory, class_name)\n#         for f in os.listdir(class_dir):\n#             if f.endswith(\".mp4\"):\n#                 video_paths.append(os.path.join(class_dir, f))\n#                 labels.append(idx)\n#     dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n#     def _parse_function(filename, label):\n#         video = tf.py_function(lambda f: load_video(f, max_frames=16, target_size=(224, 224), augment=augment, frame_stride=2), [filename], tf.float32)\n#         video.set_shape([16, 224, 224, 3])\n#         return video, label\n#     dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n#     if augment:\n#         dataset = dataset.shuffle(100)\n#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n#     return dataset\n\ndef create_dataset(directory, batch_size, class_names, augment=False, frame_stride=2):\n    video_paths, labels = [], []\n    for idx, class_name in enumerate(class_names):\n        class_dir = os.path.join(directory, class_name)\n        if os.path.exists(class_dir):\n            for f in os.listdir(class_dir):\n                if f.endswith(\".mp4\"):\n                    video_paths.append(os.path.join(class_dir, f))\n                    labels.append(idx)\n    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n    def _parse_function(filename, label):\n        video = tf.py_function(\n            lambda f: load_video(f, max_frames=16, target_size=(224, 224), augment=augment, frame_stride=frame_stride),\n            [filename],\n            tf.float32\n        )\n        video.set_shape([16, 224, 224, 3])\n        return video, label\n    dataset = dataset.shuffle(2000 if augment else 1000).cache().prefetch(tf.data.AUTOTUNE)  # Added cache and prefetch\n    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset, video_paths, labels\n\n# def create_dataset_from_paths(video_paths, labels, batch_size, class_names, augment=False, frame_stride=2):\n#     dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n#     def _parse_function(filename, label):\n#         video = tf.py_function(\n#             lambda f: load_video(f, max_frames=16, target_size=(224, 224), augment=augment, frame_stride=frame_stride),\n#             [filename],\n#             tf.float32\n#         )\n#         video.set_shape([16, 224, 224, 3])\n#         return video, label\n#     dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n#     if augment:\n#         dataset = dataset.shuffle(100)\n#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n#     return dataset\n\n\n# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n\n\n# def create_dataset_from_paths(video_paths, labels, batch_size, class_names, augment=False, frame_stride=2):\n#     dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n#     def _parse_function(filename, label):\n#         video = tf.py_function(\n#             lambda f: load_video(f, max_frames=16, target_size=(224, 224), augment=augment, frame_stride=frame_stride),\n#             [filename],\n#             tf.float32\n#         )\n#         video.set_shape([16, 224, 224, 3])\n#         return video, label\n#     dataset = dataset.shuffle(buffer_size=len(video_paths), reshuffle_each_iteration=True)  # Fixed shuffling\n#     dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n#     dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n#     dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n#     return dataset\n\n# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n\ndef create_dataset_from_paths(video_paths, labels, batch_size, class_names, augment=False, frame_stride=2):\n    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n    def _parse_function(filename, label):\n        video = tf.py_function(\n            lambda f: load_video(f, max_frames=16, target_size=(224, 224), augment=augment, frame_stride=frame_stride),\n            [filename],\n            tf.float32\n        )\n        video.set_shape([16, 224, 224, 3])\n        return video, label\n    dataset = dataset.shuffle(buffer_size=len(video_paths), reshuffle_each_iteration=True)\n    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n    dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef get_merged_dataset(train_dir, val_dir, test_dir, class_names):\n    video_paths, labels = [], []\n    for idx, class_name in enumerate(class_names):\n        # Training directory\n        train_class_dir = os.path.join(train_dir, class_name)\n        if os.path.exists(train_class_dir):\n            for f in os.listdir(train_class_dir):\n                if f.endswith(\".mp4\"):\n                    video_paths.append(os.path.join(train_class_dir, f))\n                    labels.append(idx)\n        # Validation directory\n        val_class_dir = os.path.join(val_dir, class_name)\n        if os.path.exists(val_class_dir):\n            for f in os.listdir(val_class_dir):\n                if f.endswith(\".mp4\"):\n                    video_paths.append(os.path.join(val_class_dir, f))\n                    labels.append(idx)\n        # Test directory\n        test_class_dir = os.path.join(test_dir, class_name)\n        if os.path.exists(test_class_dir):\n            for f in os.listdir(test_class_dir):\n                if f.endswith(\".mp4\"):\n                    video_paths.append(os.path.join(test_class_dir, f))\n                    labels.append(idx)\n    print(f\"Total merged dataset: {len(video_paths)} clips\")\n    print(\"Class distribution:\", {class_names[i]: sum(np.array(labels) == i) for i in range(len(class_names))})\n    return video_paths, labels\n\n\n# def visualize_clips(dataset, class_names, paths, num_clips=4):\n#     for videos, labels in dataset.take(1):\n#         indices = tf.range(len(labels)).numpy()[:num_clips]\n#         print(\"Sample validation labels:\", labels.numpy()[:num_clips])\n#         print(\"Sample validation paths:\", [paths[i] for i in indices])\n#         plt.figure(figsize=(12, 3))\n#         for i in range(min(num_clips, videos.shape[0])):\n#             plt.subplot(1, num_clips, i+1)\n#             plt.imshow(videos[i][0])\n#             plt.title(f\"Label: {class_names[labels[i]]}\")\n#             plt.axis('off')\n#         plt.show()\n\n\ndef visualize_clips(dataset, class_names, paths, num_clips=4):\n    for videos, labels in dataset.take(1):\n        indices = tf.range(len(labels)).numpy()[:num_clips]\n        print(\"Sample validation labels:\", labels.numpy()[:num_clips])\n        print(\"Sample validation paths:\", [paths[i] for i in indices])\n        plt.figure(figsize=(12, 3))\n        for i in range(min(num_clips, videos.shape[0])):\n            plt.subplot(1, num_clips, i+1)\n            plt.imshow(videos[i][0])\n            plt.title(f\"Label: {class_names[labels[i].numpy()]}\")\n            plt.axis('off')\n        plt.show()\n\ndef save_metrics(fold_metrics, filename='fold_metrics.pkl'):\n    with open(filename, 'wb') as f:\n        pickle.dump(fold_metrics, f)\n\ndef load_metrics(filename='fold_metrics.pkl'):\n    if os.path.exists(filename):\n        with open(filename, 'rb') as f:\n            return pickle.load(f)\n    return []","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:24.718955Z","iopub.execute_input":"2025-04-18T21:32:24.719157Z","iopub.status.idle":"2025-04-18T21:32:24.741840Z","shell.execute_reply.started":"2025-04-18T21:32:24.719133Z","shell.execute_reply":"2025-04-18T21:32:24.741293Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dir = \"/kaggle/input/capstone-dataset-updated/split_data_2/train\"\nval_dir = \"/kaggle/input/capstone-dataset-updated/split_data_2/val\"\ntest_dir = \"/kaggle/input/capstone-dataset-updated/split_data_2/test\"\n\nclass_names = get_class_names(train_dir)\nnum_classes = len(class_names)\nprint(\"Number of classes:\", num_classes)\n\nall_paths, all_labels = get_merged_dataset(train_dir, val_dir, test_dir, class_names)\nall_paths = np.array(all_paths)\nall_labels = np.array(all_labels)","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:24.742603Z","iopub.execute_input":"2025-04-18T21:32:24.742770Z","iopub.status.idle":"2025-04-18T21:32:24.812832Z","shell.execute_reply.started":"2025-04-18T21:32:24.742757Z","shell.execute_reply":"2025-04-18T21:32:24.812296Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['Corner', 'Direct free-kick', 'Foul', 'Goal', 'Indirect free-kick', 'Kick-off', 'Offside', 'Shots off target', 'Shots on target', 'Substitution', 'Throw-in', 'Yellow card']\nNumber of classes: 12\nTotal merged dataset: 3694 clips\nClass distribution: {'Corner': 231, 'Direct free-kick': 315, 'Foul': 264, 'Goal': 333, 'Indirect free-kick': 259, 'Kick-off': 352, 'Offside': 267, 'Shots off target': 348, 'Shots on target': 357, 'Substitution': 337, 'Throw-in': 267, 'Yellow card': 364}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# k = 1\n# skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\nfold_metrics = load_metrics()\nbatch_size = 5\nnum_frames = 16\nresolution = 224\nnum_epochs = 6\nnum_train = len(all_paths)  # ~3148\ntrain_steps = num_train // batch_size\ntotal_train_steps = train_steps * num_epochs\n\n# for fold, (train_idx, val_idx) in enumerate(skf.split(all_paths, all_labels)):\n#     if any(m['fold'] == fold + 1 for m in fold_metrics):\n#         print(f\"Skipping Fold {fold + 1}: Already completed\")\n#         continue\n        \n#     print(f\"\\nFold {fold + 1}/{k}\")\n    \n#     # Split data\n#     train_paths_fold = all_paths[train_idx]\n#     train_labels_fold = all_labels[train_idx]\n#     val_paths_fold = all_paths[val_idx]\n#     val_labels_fold = all_labels[val_idx]\n    \n#     print(f\"Training clips: {len(train_paths_fold)}\")\n#     print(f\"Validation clips: {len(val_paths_fold)}\")\n#     print(\"Validation class distribution:\", {class_names[i]: sum(val_labels_fold == i) for i in range(num_classes)})\n#     print(\"Validation labels (all):\", val_labels_fold.tolist())\n#     print(\"Unique validation labels:\", np.unique(val_labels_fold))\n    \n#     # Create datasets\n#     train_data = create_dataset_from_paths(\n#         train_paths_fold, train_labels_fold, batch_size, class_names, augment=True, frame_stride=2\n#     )\n#     val_data = create_dataset_from_paths(\n#         val_paths_fold, val_labels_fold, batch_size, class_names, augment=False, frame_stride=2\n#     )\n\n#     print(f\"Visualizing validation clips for fold {fold + 1}\")\n#     visualize_clips(val_data, class_names, val_paths_fold)\n    \n#     # Compute class weights\n#     class_weights = class_weight.compute_class_weight(\n#         'balanced', classes=np.unique(train_labels_fold), y=train_labels_fold\n#     )\n#     class_weight_dict = dict(enumerate(class_weights))\n#     if 11 in class_weight_dict:  # Reduce Yellow card weight\n#         class_weight_dict[11] *= 0.5\n#     print(f\"Class weights for fold {fold + 1}:\", class_weight_dict)\n    \n#     # Model setup\n#     tf.keras.backend.clear_session()\n#     gc.collect()\n#     # backbone = movinet.Movinet(model_id='a3')\n#     # backbone.build([1, num_frames, resolution, resolution, 3])\n    \n#     # # # Load pretrained weights for backbone\n#     # # !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a3_base.tar.gz -O movinet_a3_base.tar.gz -q\n#     # # !tar -xvf movinet_a3_base.tar.gz\n#     # # checkpoint_dir = 'movinet_a3_base'\n#     # # checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n#     # # checkpoint = tf.train.Checkpoint(backbone=backbone)\n#     # # status = checkpoint.restore(checkpoint_path).expect_partial()\n#     # # status.assert_nontrivial_match()  # Verify some weights were loaded\n    \n#     # # Build full model\n#     # model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=num_classes, dropout_rate=0.5)\n#     # model.build([batch_size, num_frames, resolution, resolution, 3])\n    \n#     # # Build classifier with partial backbone unfreezing\n#     # def build_classifier(backbone, num_classes, freeze_backbone=False):\n#     #     model = movinet_model.MovinetClassifier(\n#     #         backbone=backbone,\n#     #         num_classes=num_classes,\n#     #         dropout_rate=0.5\n#     #     )\n#     #     model.build([batch_size, num_frames, resolution, resolution, 3])\n#     #     if freeze_backbone:\n#     #         for layer in model.layers[:-1]:\n#     #             layer.trainable = False\n#     #     else:\n#     #         for layer in model.layers:\n#     #             layer.trainable = True\n#     #         for layer in model.layers[1].layers:\n#     #             if 'block3' not in layer.name and 'block4' not in layer.name:\n#     #                 layer.trainable = False\n#     #     return model\n    \n#     # model = build_classifier(backbone, num_classes, freeze_backbone=False)\n\n#     # RESUMING TRAINING FROM PREVIOUS EPOCH MODEL CODE:\n\n#     model = None  # Ensure model is not redefined if resuming\n#     if fold == 1 and os.path.exists('/kaggle/input/capstone_movinet_model_and_weights/keras/default/1/movinet_classifier_fold_2.keras'):\n#         model = tf.keras.models.load_model('/kaggle/input/capstone_movinet_model_and_weights/keras/default/1/movinet_classifier_fold_2.keras', compile=False)\n#         print(\"Resumed training from Fold 2 .keras file\")\n#         initial_epoch = 4\n#     else:\n#         backbone = movinet.Movinet(model_id='a3')\n#         backbone.build([batch_size, num_frames, resolution, resolution, 3])\n#         model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=12, dropout_rate=0.7)\n#         model.build([batch_size, num_frames, resolution, resolution, 3])\n#         initial_epoch = 0\n    \n#     # initial_learning_rate = 0.001\n#     initial_learning_rate = 0.0005\n#     total_train_steps = (len(train_paths_fold) // batch_size) * num_epochs\n#     learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n#         initial_learning_rate, decay_steps=total_train_steps\n#     )\n#     model.compile(\n#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#         optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0),\n#         metrics=['accuracy'],\n#         jit_compile=True\n#     )\n    \n#     # Callbacks\n#     callbacks = [\n#         tf.keras.callbacks.TensorBoard(log_dir=f'./logs/fold_{fold + 1}'),\n#         tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n#         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n#     ]\n    \n#     # Train model\n#     history = model.fit(\n#         train_data,\n#         validation_data=val_data,\n#         epochs=num_epochs,\n#         initial_epoch=4,\n#         validation_freq=1,\n#         verbose=1,\n#         callbacks=callbacks,\n#         class_weight=class_weight_dict\n#     )\n    \n#     # Evaluate and store metrics\n#     val_loss, val_accuracy = model.evaluate(val_data, verbose=0)\n#     fold_metrics.append({'fold': fold + 1, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n#     print(f\"Fold {fold + 1} - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n    \n#     # Debugging: Check predictions\n#     val_preds = model.predict(val_data, verbose=0)\n#     val_preds_classes = np.argmax(val_preds, axis=1)\n#     print(f\"Fold {fold + 1} - Predicted class distribution:\", {class_names[i]: sum(val_preds_classes == i) for i in range(num_classes)})  \n#     print(f\"Fold {fold + 1} - True class distribution:\", {class_names[i]: sum(val_labels_fold == i) for i in range(num_classes)})  \n\n#     cm = confusion_matrix(val_labels_fold, val_preds_classes)\n#     plt.figure(figsize=(10, 8))\n#     sns.heatmap(\n#         cm / cm.sum(axis=1)[:, np.newaxis],  # Normalize by true class counts\n#         annot=True,\n#         fmt='.2f',\n#         cmap='Blues',\n#         xticklabels=class_names,\n#         yticklabels=class_names\n#     )\n#     plt.title(f'Confusion Matrix - Fold {fold + 1}')\n#     plt.xlabel('Predicted')\n#     plt.ylabel('True')\n#     plt.savefig(f'confusion_matrix_fold_{fold + 1}.png')  # Save for reference\n#     plt.show()\n\n    \n#     # Save model\n#     save_metrics(fold_metrics)\n#     model.save(f\"movinet_classifier_fold_{fold + 1}.keras\")\n#     model.save_weights(f\"movinet_classifier_weights_fold_{fold + 1}.weights.h5\")\n\n#     del model, train_data, val_data\n#     tf.keras.backend.clear_session()\n#     gc.collect()\n#     !nvidia-smi\n\n# # Average metrics\n# avg_val_loss = np.mean([m['val_loss'] for m in fold_metrics])\n# avg_val_accuracy = np.mean([m['val_accuracy'] for m in fold_metrics])\n# print(f\"\\nAverage across {k} folds - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    all_paths, all_labels, test_size=0.3, stratify=all_labels, random_state=42\n)\n\nprint(f\"\\nFold 1/1\")\nprint(f\"Training clips: {len(train_paths)}\")\nprint(f\"Validation clips: {len(val_paths)}\")\nprint(\"Validation class distribution:\", {class_names[i]: sum(val_labels == i) for i in range(num_classes)})\nprint(\"Validation labels (all):\", val_labels.tolist())\nprint(\"Unique validation labels:\", np.unique(val_labels))\n\n# Create datasets\ntrain_data = create_dataset_from_paths(\n    train_paths, train_labels, batch_size, class_names, augment=True, frame_stride=2\n)\nval_data = create_dataset_from_paths(\n    val_paths, val_labels, batch_size, class_names, augment=False, frame_stride=2\n)\n\nprint(f\"Visualizing validation clips for fold 1\")\nvisualize_clips(val_data, class_names, val_paths)\n\n# Compute class weights\nclass_weights = class_weight.compute_class_weight(\n    'balanced', classes=np.unique(train_labels), y=train_labels\n)\nclass_weight_dict = dict(enumerate(class_weights))\nif 11 in class_weight_dict:  # Reduce Yellow card weight\n    class_weight_dict[11] *= 0.5\nprint(f\"Class weights for fold 1:\", class_weight_dict)\n\n# Model setup\ntf.keras.backend.clear_session()\ngc.collect()\n\n# Rebuild model and load weights\nbackbone = movinet.Movinet(model_id='a3')\nbackbone.build([batch_size, num_frames, resolution, resolution, 3])\nmodel = movinet_model.MovinetClassifier(\n    backbone=backbone,\n    num_classes=num_classes,\n    dropout_rate=0.7\n)\nmodel.build([batch_size, num_frames, resolution, resolution, 3])\nif os.path.exists('/kaggle/input/capstone-thirditeration-model/keras/default/1/fold_1_epoch_4.weights.h5'):\n    model.load_weights('/kaggle/input/capstone-thirditeration-model/keras/default/1/fold_1_epoch_4.weights.h5')\n    print(\"Resumed training from Fold 1 .weights.h5 file\")\n    # initial_epoch = 4  # Resume from epoch 4\nelse:\n    print(\"No weights file found, starting from scratch\")\n    # initial_epoch = 0\n\ninitial_learning_rate = 0.0005\nlearning_rate = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate, decay_steps=total_train_steps\n)\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0),\n    metrics=['accuracy'],\n    jit_compile=True\n)\n\n# Callbacks\ncallbacks = [\n    tf.keras.callbacks.TensorBoard(log_dir=f'./logs/fold_1'),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6),\n    tf.keras.callbacks.ModelCheckpoint(filepath=f\"iter_4_epoch_{{epoch}}.weights.h5\", save_weights_only=True, save_best_only=False, verbose=1)\n]\n\n# Train model\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=num_epochs,\n    validation_freq=1,\n    verbose=1,\n    callbacks=callbacks,\n    class_weight=class_weight_dict\n)\n\n# Evaluate and store metrics\nval_loss, val_accuracy = model.evaluate(val_data, verbose=0)\nfold_metrics.append({'fold': 1, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\nprint(f\"Fold 1 - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n# Debugging: Check predictions\nval_preds = model.predict(val_data, verbose=0)\nval_preds_classes = np.argmax(val_preds, axis=1)\nprint(f\"Fold 1 - Predicted class distribution:\", {class_names[i]: sum(val_preds_classes == i) for i in range(num_classes)})\nprint(f\"Fold 1 - True class distribution:\", {class_names[i]: sum(val_labels == i) for i in range(num_classes)})\n\ncm = confusion_matrix(val_labels, val_preds_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    cm / cm.sum(axis=1)[:, np.newaxis],\n    annot=True,\n    fmt='.2f',\n    cmap='Blues',\n    xticklabels=class_names,\n    yticklabels=class_names\n)\nplt.title(f'Confusion Matrix - Fold 1')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig(f'confusion_matrix_epochs_50.png')\nplt.show()\n\n# Save model\nsave_metrics(fold_metrics)\nmodel.save(f\"movinet_classifier_epoch_10.keras\")\nmodel.save_weights(f\"movinet_classifier_weights_epoch_10.weights.h5\")\n\ndel model, train_data, val_data\ngc.collect()\n!nvidia-smi\n\n# Average metrics\navg_val_loss = np.mean([m['val_loss'] for m in fold_metrics])\navg_val_accuracy = np.mean([m['val_accuracy'] for m in fold_metrics])\nprint(f\"\\nAverage across 1 folds - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-18T21:32:24.813437Z","iopub.execute_input":"2025-04-18T21:32:24.813642Z","iopub.status.idle":"2025-04-18T22:21:15.358883Z","shell.execute_reply.started":"2025-04-18T21:32:24.813622Z","shell.execute_reply":"2025-04-18T22:21:15.357942Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nFold 1/1\nTraining clips: 2585\nValidation clips: 1109\nValidation class distribution: {'Corner': 69, 'Direct free-kick': 95, 'Foul': 79, 'Goal': 100, 'Indirect free-kick': 78, 'Kick-off': 106, 'Offside': 80, 'Shots off target': 105, 'Shots on target': 107, 'Substitution': 101, 'Throw-in': 80, 'Yellow card': 109}\nValidation labels (all): [7, 7, 8, 2, 11, 0, 0, 7, 9, 1, 5, 3, 7, 1, 6, 0, 6, 5, 8, 2, 8, 10, 10, 2, 8, 3, 6, 6, 9, 6, 4, 0, 8, 1, 6, 9, 10, 8, 7, 0, 5, 10, 11, 1, 4, 3, 0, 11, 8, 11, 1, 3, 8, 3, 11, 11, 6, 9, 6, 8, 8, 5, 11, 1, 8, 6, 5, 0, 11, 6, 8, 2, 10, 8, 11, 11, 1, 5, 11, 9, 11, 0, 7, 5, 5, 7, 7, 4, 3, 7, 11, 1, 9, 5, 8, 5, 9, 4, 2, 1, 9, 3, 11, 2, 6, 5, 1, 6, 8, 8, 5, 7, 8, 9, 11, 11, 0, 11, 1, 10, 5, 5, 6, 11, 4, 7, 11, 7, 8, 7, 5, 4, 7, 7, 0, 9, 5, 5, 9, 1, 0, 8, 3, 0, 4, 11, 9, 8, 11, 3, 3, 0, 9, 2, 4, 7, 4, 9, 8, 8, 2, 2, 6, 10, 4, 7, 2, 0, 7, 11, 3, 7, 4, 6, 3, 9, 5, 10, 6, 0, 6, 5, 5, 11, 2, 8, 5, 1, 3, 5, 1, 5, 7, 3, 5, 11, 3, 11, 2, 0, 7, 11, 4, 9, 2, 8, 8, 5, 0, 1, 2, 2, 0, 10, 9, 2, 7, 10, 10, 7, 7, 5, 0, 1, 9, 1, 5, 3, 6, 9, 11, 9, 6, 3, 10, 3, 4, 5, 11, 6, 4, 7, 4, 9, 7, 7, 0, 3, 3, 7, 4, 1, 9, 11, 3, 1, 8, 11, 8, 5, 1, 5, 5, 5, 5, 4, 10, 3, 1, 3, 8, 3, 4, 7, 6, 3, 8, 10, 11, 2, 9, 0, 10, 0, 8, 8, 5, 8, 11, 3, 0, 3, 6, 8, 9, 2, 2, 10, 10, 8, 9, 2, 8, 1, 8, 2, 4, 6, 1, 7, 1, 7, 1, 8, 1, 3, 6, 10, 1, 11, 3, 11, 4, 9, 0, 11, 11, 4, 5, 5, 3, 5, 8, 2, 10, 9, 11, 3, 4, 2, 0, 11, 11, 9, 11, 6, 1, 2, 2, 2, 8, 8, 4, 11, 11, 0, 1, 1, 7, 10, 4, 11, 5, 4, 7, 2, 4, 3, 2, 4, 8, 0, 7, 8, 11, 4, 4, 9, 8, 11, 0, 7, 2, 9, 8, 9, 7, 9, 9, 6, 7, 3, 3, 2, 9, 3, 0, 4, 0, 7, 1, 9, 6, 7, 5, 4, 4, 10, 9, 5, 10, 8, 4, 8, 4, 1, 8, 10, 9, 11, 8, 6, 4, 1, 3, 9, 0, 2, 10, 10, 5, 9, 2, 2, 7, 3, 4, 7, 10, 8, 8, 11, 10, 3, 1, 1, 3, 0, 8, 3, 7, 11, 5, 7, 5, 3, 10, 10, 3, 5, 3, 8, 0, 11, 11, 11, 6, 7, 6, 2, 1, 10, 4, 4, 9, 0, 4, 8, 3, 4, 2, 10, 5, 1, 4, 11, 9, 8, 0, 6, 7, 4, 10, 11, 6, 5, 11, 1, 0, 4, 9, 11, 5, 9, 3, 4, 1, 3, 4, 8, 0, 5, 10, 5, 7, 0, 3, 4, 11, 6, 1, 8, 5, 10, 7, 1, 0, 6, 6, 0, 4, 3, 1, 9, 5, 10, 8, 4, 2, 6, 2, 5, 3, 11, 7, 3, 3, 11, 3, 7, 4, 3, 3, 7, 11, 11, 1, 5, 3, 6, 1, 11, 10, 8, 5, 9, 11, 5, 3, 1, 8, 11, 8, 6, 7, 8, 2, 9, 3, 1, 11, 5, 9, 2, 7, 11, 9, 1, 3, 7, 6, 8, 9, 4, 3, 0, 3, 4, 8, 0, 2, 6, 7, 5, 1, 5, 7, 7, 1, 9, 8, 10, 6, 11, 1, 8, 2, 10, 1, 11, 4, 4, 3, 9, 9, 5, 10, 1, 6, 11, 6, 10, 5, 8, 3, 8, 1, 3, 2, 10, 4, 7, 9, 3, 11, 10, 9, 9, 8, 1, 4, 5, 4, 5, 0, 2, 1, 5, 2, 1, 6, 7, 11, 5, 5, 11, 10, 11, 9, 2, 8, 3, 7, 8, 10, 11, 2, 9, 9, 7, 9, 5, 9, 6, 9, 7, 11, 7, 1, 8, 11, 8, 7, 2, 1, 7, 8, 1, 11, 2, 11, 6, 10, 10, 0, 11, 3, 0, 3, 5, 6, 11, 7, 2, 9, 10, 8, 6, 9, 6, 1, 10, 8, 7, 9, 9, 8, 10, 10, 2, 8, 8, 6, 7, 11, 8, 11, 0, 8, 5, 4, 8, 3, 11, 0, 2, 10, 0, 5, 5, 1, 10, 1, 7, 10, 1, 9, 5, 0, 0, 7, 7, 8, 0, 6, 2, 5, 1, 1, 9, 7, 4, 6, 7, 8, 11, 10, 6, 9, 9, 6, 7, 0, 0, 5, 10, 2, 11, 3, 7, 10, 1, 10, 6, 10, 9, 9, 1, 2, 5, 4, 3, 10, 1, 8, 4, 1, 1, 6, 7, 6, 5, 9, 4, 1, 2, 5, 6, 3, 6, 11, 10, 1, 9, 11, 6, 4, 2, 9, 0, 4, 6, 11, 2, 7, 7, 5, 3, 5, 8, 5, 5, 8, 4, 0, 8, 5, 0, 2, 5, 5, 9, 3, 8, 7, 1, 3, 1, 9, 11, 9, 8, 8, 5, 2, 7, 10, 10, 2, 3, 2, 2, 1, 7, 0, 5, 0, 10, 0, 5, 9, 9, 6, 8, 11, 3, 11, 7, 2, 11, 1, 1, 3, 1, 6, 0, 7, 11, 10, 1, 3, 10, 11, 5, 1, 9, 3, 6, 7, 9, 3, 5, 1, 5, 7, 3, 8, 9, 3, 7, 9, 3, 9, 7, 5, 7, 8, 10, 11, 7, 8, 6, 5, 7, 0, 11, 6, 4, 9, 5, 10, 10, 1, 1, 5, 5, 9, 3, 10, 7, 1, 7, 3, 8, 5, 3, 0, 8, 5, 6, 3, 1, 1, 6, 6, 4, 11, 4, 3, 4, 4, 0, 6, 10, 8, 7, 10, 5, 11, 11, 1, 1, 4, 9, 3, 2, 11, 5, 11, 11, 8, 8, 9, 2, 3, 10, 7, 4, 5, 10, 9, 4, 11, 6, 7, 11, 6, 8, 3, 8, 6, 3, 0, 6, 1, 3, 5, 8, 5, 0, 9, 7, 7, 0, 9, 4, 5, 1, 7, 4, 0, 2, 11, 4, 2, 6, 7, 9, 2, 10, 5, 0, 3, 7, 2, 4, 4, 7, 2, 7, 9, 2, 9, 6, 0, 7, 10, 1, 9, 11, 3, 7, 3, 1, 2, 9, 5, 6, 6, 11, 10, 2, 6, 2, 11, 1, 3, 5, 9, 10, 3, 2, 10, 2, 11, 3, 2, 4, 1, 9, 10, 10, 3, 9, 9, 6, 7, 8, 9, 11, 7, 5, 2, 8, 8, 8, 1, 8, 2]\nUnique validation labels: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745011945.074108   68357 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Visualizing validation clips for fold 1\nSample validation labels: [3 1 8 3]\nSample validation paths: ['/kaggle/input/capstone-dataset-updated/split_data_2/train/Shots off target/aug_6102_Shots off target_149.mp4', '/kaggle/input/capstone-dataset-updated/split_data_2/train/Shots off target/Shots off target_184.mp4', '/kaggle/input/capstone-dataset-updated/split_data_2/train/Shots on target/Shots on target_38.mp4', '/kaggle/input/capstone-dataset-updated/split_data_2/test/Foul/Foul_188.mp4']\nClass weights for fold 1: {0: 1.329732510288066, 1: 0.9791666666666666, 2: 1.1644144144144144, 3: 0.9245350500715308, 4: 1.1901473296500922, 5: 0.8756775067750677, 6: 1.1519607843137254, 7: 0.8864883401920439, 8: 0.8616666666666667, 9: 0.9127824858757062, 10: 1.1519607843137254, 11: 0.4223856209150327}\nResumed training from Fold 1 .weights.h5 file\nEpoch 1/6\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745012075.657976   68420 service.cc:152] XLA service 0x7e9e98255ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745012075.658018   68420 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745012100.588023   68420 cuda_dnn.cc:529] Loaded cuDNN version 90300\n2025-04-18 21:35:02.839311: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 1s:\n\n  %reduce-window.10147 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.10136, f32[] %constant.10137), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.10143, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:35:47.247287: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 45.410435438s\nConstant folding an instruction is taking > 1s:\n\n  %reduce-window.10147 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.10136, f32[] %constant.10137), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.10143, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:35:49.258305: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 2s:\n\n  %reduce-window.14332 = f32[5,16,28,28,16]{4,3,2,1,0} reduce-window(f32[5,16,56,56,16]{4,3,2,1,0} %broadcast.14321, f32[] %constant.14322), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.14328, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block1_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:35:58.255900: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 11.000344616s\nConstant folding an instruction is taking > 2s:\n\n  %reduce-window.14332 = f32[5,16,28,28,16]{4,3,2,1,0} reduce-window(f32[5,16,56,56,16]{4,3,2,1,0} %broadcast.14321, f32[] %constant.14322), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.14328, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block1_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:36:02.261303: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 4s:\n\n  %reduce-window.20567 = f32[5,16,14,14,48]{4,3,2,1,0} reduce-window(f32[5,16,28,28,48]{4,3,2,1,0} %broadcast.20556, f32[] %constant.20557), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.20563, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block2_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:36:06.263521: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 8.003311801s\nConstant folding an instruction is taking > 4s:\n\n  %reduce-window.20567 = f32[5,16,14,14,48]{4,3,2,1,0} reduce-window(f32[5,16,28,28,48]{4,3,2,1,0} %broadcast.20556, f32[] %constant.20557), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.20563, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block2_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\nI0000 00:00:1745012239.912085   68420 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"517/517 [==============================] - ETA: 0s - loss: 1.9545 - accuracy: 0.2662","output_type":"stream"},{"name":"stderr","text":"2025-04-18 21:44:19.862309: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 8s:\n\n  %reduce-window.1704 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.1693, f32[] %constant.1694), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1700, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:44:56.416437: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 44.557017835s\nConstant folding an instruction is taking > 8s:\n\n  %reduce-window.1704 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.1693, f32[] %constant.1694), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1700, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:47:30.533308: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 16s:\n\n  %reduce-window.1704 = f32[4,16,56,56,16]{4,3,2,1,0} reduce-window(f32[4,16,112,112,16]{4,3,2,1,0} %broadcast.1693, f32[] %constant.1694), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1700, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 21:47:49.891825: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 35.361024616s\nConstant folding an instruction is taking > 16s:\n\n  %reduce-window.1704 = f32[4,16,56,56,16]{4,3,2,1,0} reduce-window(f32[4,16,112,112,16]{4,3,2,1,0} %broadcast.1693, f32[] %constant.1694), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1700, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: saving model to iter_4_epoch_1.weights.h5\n517/517 [==============================] - 932s 1s/step - loss: 1.9545 - accuracy: 0.2662 - val_loss: 2.0828 - val_accuracy: 0.2543 - lr: 4.8343e-04\nEpoch 2/6\n517/517 [==============================] - ETA: 0s - loss: 1.7722 - accuracy: 0.3164\nEpoch 2: saving model to iter_4_epoch_2.weights.h5\n517/517 [==============================] - 547s 1s/step - loss: 1.7722 - accuracy: 0.3164 - val_loss: 2.1693 - val_accuracy: 0.2678 - lr: 4.3581e-04\nEpoch 3/6\n517/517 [==============================] - ETA: 0s - loss: 1.5187 - accuracy: 0.4108\nEpoch 3: saving model to iter_4_epoch_3.weights.h5\n517/517 [==============================] - 498s 964ms/step - loss: 1.5187 - accuracy: 0.4108 - val_loss: 2.3897 - val_accuracy: 0.2642 - lr: 3.6347e-04\nEpoch 4/6\n517/517 [==============================] - ETA: 0s - loss: 1.1982 - accuracy: 0.5261\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.0001380131288897246.\n\nEpoch 4: saving model to iter_4_epoch_4.weights.h5\n517/517 [==============================] - 551s 1s/step - loss: 1.1982 - accuracy: 0.5261 - val_loss: 2.8678 - val_accuracy: 0.2408 - lr: 2.7603e-04\nEpoch 4: early stopping\nRestoring model weights from the end of the best epoch: 1.\nFold 1 - Val Loss: 2.0828, Val Accuracy: 0.2543\n","output_type":"stream"},{"name":"stderr","text":"2025-04-18 22:17:40.046336: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 32s:\n\n  %reduce-window.1696 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.1685, f32[] %constant.1686), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1692, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n2025-04-18 22:17:53.698953: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 45.655184913s\nConstant folding an instruction is taking > 32s:\n\n  %reduce-window.1696 = f32[5,16,56,56,16]{4,3,2,1,0} reduce-window(f32[5,16,112,112,16]{4,3,2,1,0} %broadcast.1685, f32[] %constant.1686), window={size=1x1x3x3x1 stride=1x1x2x2x1 pad=0_0x0_0x0_1x0_1x0_0}, to_apply=%region_1.1692, metadata={op_type=\"AvgPool3D\" op_name=\"movinet_classifier/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D\"}\n\nThis isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n\nIf you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Predicted class distribution: {'Corner': 43, 'Direct free-kick': 51, 'Foul': 104, 'Goal': 144, 'Indirect free-kick': 102, 'Kick-off': 77, 'Offside': 137, 'Shots off target': 4, 'Shots on target': 169, 'Substitution': 134, 'Throw-in': 127, 'Yellow card': 17}\nFold 1 - True class distribution: {'Corner': 69, 'Direct free-kick': 95, 'Foul': 79, 'Goal': 100, 'Indirect free-kick': 78, 'Kick-off': 106, 'Offside': 80, 'Shots off target': 105, 'Shots on target': 107, 'Substitution': 101, 'Throw-in': 80, 'Yellow card': 109}\nFri Apr 18 22:21:15 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             32W /  250W |   15869MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nAverage across 1 folds - Val Loss: 2.1484, Val Accuracy: 0.2326\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport zipfile\n\n# List files to include\nfiles_to_zip = ['confusion_matrix_fold_1.png', 'movinet_classifier_weights_fold_1.weights.h5','movinet_classifier_fold_1.keras']\n# files_to_zip = ['confusion_matrix_fold_1.png', 'confusion_matrix_fold_2.png', 'movinet_classifier_fold_1.keras', 'movinet_classifier_fold_2.keras','movinet_classifier_weights_fold_1.weights.h5', 'movinet_classifier_weights_fold_2.weights.h5']\noutput_zip = 'outputs.zip'\n\n# Create a zip file\nwith zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for file in files_to_zip:\n        if os.path.exists(file):\n            zipf.write(file)\n        else:\n            print(f\"Warning: {file} not found\")\n\n# Provide download link\nfrom IPython.display import FileLink\nFileLink(output_zip)","metadata":{"execution":{"iopub.status.busy":"2025-04-18T22:21:15.359901Z","iopub.execute_input":"2025-04-18T22:21:15.360144Z","iopub.status.idle":"2025-04-18T22:21:22.570138Z","shell.execute_reply.started":"2025-04-18T22:21:15.360119Z","shell.execute_reply":"2025-04-18T22:21:22.569369Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/outputs.zip","text/html":"<a href='outputs.zip' target='_blank'>outputs.zip</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# **END OF LATEST CODE**","metadata":{}}]}